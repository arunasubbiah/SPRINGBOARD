{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytz\n",
    "from datetime import timedelta, date, datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1674.39 s\n"
     ]
    }
   ],
   "source": [
    "#Commented code below reads data from all the 62 files and appends them together into a single dataframe.\n",
    "#Performing indexing & grouping operations on this huge dataframe takes about 1674.39s ~30 min\n",
    "#results in a very bad performance. A better approach is to read data from each file, perform indexing & grouping\n",
    "#and then concat\n",
    "'''\n",
    "start_time = time.time()\n",
    "\n",
    "#Initialize an empty dataframe to append the daywise resampled data\n",
    "dayDF = pd.DataFrame()\n",
    "\n",
    "filename_constant = \"Data/sms-call-internet-mi-{}.txt\"\n",
    "col_list = ['cellID', 'timeInterval', 'countryCode', 'smsIn', 'smsOut', 'callIn', 'callOut', 'internetActivity']\n",
    "\n",
    "#Generating filenames iteratively\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "start_date = date(2013, 11, 1)\n",
    "end_date = date(2014, 1, 2)\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    read_data = pd.read_csv(filename_constant.format(single_date.strftime(\"%Y-%m-%d\")), sep='\\t',header=None, names=col_list, parse_dates=True)\n",
    "    #Convert timeInterval column which has Unix timestamps to datetime object\n",
    "    read_data['startTime'] = pd.to_datetime(read_data.timeInterval, unit='ms')\n",
    "    \n",
    "    #Drop timeInterval column as it is now redundant\n",
    "    read_data.drop(columns=['timeInterval','countryCode'], inplace=True)\n",
    "    \n",
    "    #Group the columns by startTime and cellID\n",
    "    #data_grouped = read_data.groupby(['startTime', 'cellID'])['smsIn','smsOut','callIn','callOut','internetActivity'].sum()\n",
    "\n",
    "    #data_grouped = read_data.groupby([Grouper(key='startTime', freq='D'), 'cellID'])['smsIn','smsOut','callIn','callOut','internetActivity'].sum()\n",
    "    #Resample it daywise and get the total sum of the values\n",
    "    #data_resample = data_grouped.resample('D', level=0).sum()\n",
    "    dayDF = dayDF.append(read_data)\n",
    "#Set multi-level index on cellID & startTime\n",
    "dayDF = dayDF.set_index(['cellID','startTime'])  \n",
    "#now use grouper\n",
    "\n",
    "dayDF = dayDF.groupby([pd.Grouper(level='cellID'), pd.Grouper(level='startTime', freq='D')]).sum()\n",
    "print('%3.2f s' %(time.time() - start_time))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read 62 data files that has volumes proportional to telecommunication activities (SMS, Call, Internet) that occurred every 10 min. Aggregate to hourly and daily volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515.67 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#Initialize an empty dataframe to append the daily and hourly resampled data\n",
    "dailyGridActivity = pd.DataFrame()\n",
    "hourlyGridActivity = pd.DataFrame()\n",
    "\n",
    "filename_constant = \"Data/sms-call-internet-mi-{}.txt\"\n",
    "col_list = ['gridID', 'timeInterval', 'countryCode', 'smsIn', 'smsOut', 'callIn', 'callOut', 'internetActivity']\n",
    "\n",
    "#Generating filenames iteratively\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "start_date = date(2013, 11, 1)\n",
    "end_date = date(2014, 1, 2)\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    read_data = pd.read_csv(filename_constant.format(single_date.strftime(\"%Y-%m-%d\")), sep='\\t',header=None, names=col_list, parse_dates=True)\n",
    "    \n",
    "    #Check for null values (Execute until this point seperately before proceeding ahead)\n",
    "    #print(filename_constant.format(single_date.strftime(\"%Y-%m-%d\")))\n",
    "    #print(read_data.isnull().sum())\n",
    "    \n",
    "    #Convert timeInterval column which has Epoch timestamps to UTC and then convert to Milan's local timezone\n",
    "    #tz_localize(None) returns local time format, not \"UTC+1:00\" format\n",
    "    read_data['startTime'] = pd.to_datetime(read_data.timeInterval, unit='ms', utc=True).dt.tz_convert('CET').dt.tz_localize(None)\n",
    "    \n",
    "    #Drop timeInterval columns\n",
    "    read_data.drop(columns=['timeInterval','countryCode'], inplace=True)\n",
    "    \n",
    "    #Groupby gridID and startTime as well as set index \n",
    "    #startTime which is 10 min apart is resampled to daily aggregation \n",
    "    read_data_daily = read_data.groupby(['gridID', pd.Grouper(key='startTime', freq='D')]).sum()\n",
    "    dailyGridActivity = pd.concat([dailyGridActivity,read_data_daily]).groupby(['gridID', 'startTime']).sum()\n",
    "    \n",
    "    #Groupby gridID and startTime as well as set index \n",
    "    #startTime which is 10 min apart is resampled to hourly aggregation \n",
    "    read_data_hourly = read_data.groupby(['gridID', pd.Grouper(key='startTime', freq='H')]).sum()\n",
    "    hourlyGridActivity = pd.concat([hourlyGridActivity,read_data_hourly]).groupby(['gridID', 'startTime']).sum()\n",
    "    \n",
    "print('%3.2f s' %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the geojson file to map the Grid Id over the city of Milan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://geojson.io/#id=gist:/a0e26a2d5afe5502da2fd756dd9f7053'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas\n",
    "import geojsonio\n",
    "milan = geopandas.read_file('Data/milano-grid.geojson')\n",
    "milan = milan.to_json()\n",
    "geojsonio.display(milan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![10000 grids spatially distributed over city of Milan](images/City_of_Milan.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
